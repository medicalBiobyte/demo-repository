import logging
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.runnables import RunnableConfig
from langchain_google_genai import ChatGoogleGenerativeAI
import google.generativeai as genai
from tavily import TavilyClient

# ğŸ” .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")
genai.configure(api_key=GOOGLE_API_KEY)

# Logging ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# LLM ë° ë²¡í„° ì„ë² ë”© ì„¤ì •
text_llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)

image_llm = genai.GenerativeModel("gemini-1.5-flash")
web_search_llm = TavilyClient(api_key=TAVILY_API_KEY)

embeddings = OpenAIEmbeddings(model="text-embedding-3-small", dimensions=1536)

vector_store = Chroma(
    collection_name="my_docs",
    embedding_function=embeddings,
    persist_directory="./chroma_db",
)

if __name__ == "__main__":
    test_prompt = "íŒŒì´ì¬ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"

    # LangSmith ì¶”ì  êµ¬ì„±
    config = RunnableConfig(configurable={"session_name": "graph_medi_test"})

    # LLM í˜¸ì¶œ (ìˆ˜ì •ë¨)
    response = text_llm.invoke(test_prompt, config=config)

    # ê²°ê³¼ ì¶œë ¥
    print("\n=== LLM í…ŒìŠ¤íŠ¸ ===")
    print(f"ì§ˆë¬¸: {test_prompt}")
    print(f"ì‘ë‹µ: {response.content}")
